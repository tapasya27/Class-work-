{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO 103: Introduction to data science <br> Demo \\#3: APIs<br> Author: JRW\n",
    "## Mission\n",
    "In this work book we're going to take a look at how APIs really work from a programming point of view to gain insight into how they are used to build online applications. \n",
    "\n",
    "1. Build our own API client for the Facebook Graph API to:\n",
    "    - download user posts,\n",
    "    - extract posted images, and\n",
    "    - gather a stream of user comments.\n",
    "2. Use a well-developed Twitter API client to:\n",
    "    - download historical (famous) tweets by ID,\n",
    "    - download a specific twitter users recent timeline of tweets, and\n",
    "    - filter a live stream of tweets by key words and locations.\n",
    "3. Use a well-developed Google API client to:\n",
    "    - find geographic information from a street address,\n",
    "    - find a street address from a latitude/longitude pair, and\n",
    "    - find directions between two places by name.\n",
    "4. Our own local SEPTA!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os, re\n",
    "from IPython.core.display import display\n",
    "from PIL import Image\n",
    "from io import StringIO\n",
    "import urllib.request\n",
    "from urllib.request import urlopen\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quotas\n",
    "Remember, APIs are not usually free. They will just about always come with a liscence and the way most of the sites enforce a paywall is with a rate limit or quota. \n",
    "\n",
    "#### Graph API limits\n",
    "Facebook's is so hard to hit that you may never notice (1 query/second), but they reserve the much more massive trove of private data they have. \n",
    "\n",
    "#### Twitter API limits\n",
    "Twitter will let you see pretty much any of their data, but cap you at a 1% stream limit, or at 180 calls per 15 minute window if you are using the rest API.\n",
    "\n",
    "#### Geocoding quotas\n",
    "Users of the standard API:\n",
    "\n",
    "2,500 free requests per day, calculated as the sum of client-side and server-side queries.\n",
    "50 requests per second, calculated as the sum of client-side and server-side queries.\n",
    "\n",
    "#### Directions quotas\n",
    "Users of the standard API:\n",
    "\n",
    "2,500 free directions requests per day, calculated as the sum of client-side and server-side queries.\n",
    "Up to 23 waypoints allowed in each request, whether client-side or server-side queries.\n",
    "50 requests per second, calculated as the sum of client-side and server-side queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facebook\n",
    "#### What is the Facebook Graph API\n",
    "Data from Facebook comes from the 'Graph' API, because that view their platform as a network, or, 'graph'. The documentation for this API may be found at:\n",
    "\n",
    "* https://developers.facebook.com/docs/graph-api\n",
    "\n",
    "The graph API allows you to access information about specific individuals, friends, and posts, etc. Anyone with a Facebook account can access the Graph API, and there are other APIs, such as the Public Feed API:\n",
    "\n",
    "* https://developers.facebook.com/docs/public_feed/\n",
    "\n",
    "which provides streaming data, i.e., live emerging data. However, this API is really only available to a restricted set of users, so we will focus only on the Graph API. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting a Facebook app ID\n",
    "\n",
    "As mentioned, you can use the Graph API if you are on Facebook, but to do this you have to register as a developer. As usual, there are some helpful resources out there on stackoverflow:\n",
    "\n",
    "* http://stackoverflow.com/questions/3203649/where-can-i-find-my-facebook-application-id-and-secret-key\n",
    "\n",
    "Here, I would say that the most helpful suggestion directs to the app registration page. Create an app:\n",
    "\n",
    "* https://developers.facebook.com/apps\n",
    "\n",
    "After you create an app, you will wind up on the app's development page. At the top of this page is your App ID. Record your ID in the string here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_ID = \"440014060125609\"\n",
    "APP_SECRET = \"9ab6ed302f08aa3e6b6a19225de26ee6\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will also need to get you App's secret code. This may be obtained by navigating to \"settings\" in the navigation on the left side of the app development. Once there, you will have to click on the \"show\" button to see the secret code. Record this string here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building API requests as URL strings\n",
    "Api requests on both of Facebook and twitter are really just URLs. This makes sense, because whenever you look at a webpage you are actually just downloading its content. There are a lot of details on the Facebook Graph API, and we're just going to build one kind of request: the last $N$ public posts of a particular user. The following function creates a request URL from several inputs, notably the user's name (username) and the number of past messages to collect (limit). The `APP_ID` and `APP_Secret` are both passed to this function, as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createPostUrl(username, APP_ID, APP_SECRET, limit):\n",
    "    post_args = \"/feed?access_token=\" + APP_ID + \"|\" + APP_SECRET + \\\n",
    "    \"&fields=attachments,created_time,message&limit=\" + str(limit)\n",
    "    post_url = \"https://graph.facebook.com/\" + username + post_args\n",
    "    return post_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requesting the data behind a URL\n",
    "This is the function that really does all of the work, relying on the globally-assigned `APP_ID` and `APP_SECRET`. This function runs the `CreatePostUrl()` function and then makes the http request with the `urllib2.urlopen()` function. The web response is read, and appears as a string, which, in JSON format is converted to a python dictionary using the `json.loads()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPosts(username, limit):\n",
    "    post_url = createPostUrl(username, APP_ID, APP_SECRET, limit)\n",
    "    web_response = urllib.request.urlopen(post_url)\n",
    "    readable_page = web_response.read()\n",
    "    return json.loads(readable_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the API function\n",
    "Let's try this out and grab the last 10 posts made by Drexel university (`'drexeluniv'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "data = getPosts(\"drexeluniv\", 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspecting the output\n",
    "The resulting data object is a dictionary at the top level with two keys, `'paging'`, and `'data'`. The value of `'data'` is what we're really looking for, and `'paging'` is actually another post URL that helps us to go even further back in time. In other words, we only asked for the 10 most recent posts, and if we want the ten before those, we just use the URL in `data['paging']`. Check it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['paging']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The actual data\n",
    "That's ugly, but it's really important if we want to way back in time and great that we don't have to build it. The actualy data it self is under the `'data'` key, and is a list of the different posts. Let's look at the first (most recent) post:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['data'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are the individual pieces of data we requested?\n",
    "In addition to the post message, the URL requests we built include any attachments and the creation time. The creation time `'created_time'` is fairly straightforward, but the attachments include any images that were in the post. Here's the primary message, itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['data'][0]['message']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What if we want to see the photo?\n",
    "The attachments key has another dictionary as value, let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['data'][0]['attachments']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dictionary holds another dictionary with only one key, `'data'`, whose value is a list containing all of the meat. It's a list because the post may have multiple attachments! There's only one here, and it has a `'description'` and `'title'`, a `'url'` to the linked Drexel website and not the actual image. To get the actual image, we need to look at the `'media'` key under `'image'` and then `'src'`. Follow this link with your browser and you'll see the image that Drexel posted, which is of Berlin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['data'][0]['attachments']['data'][0]['media']['image']['src']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What if we want to download the image?\n",
    "Well, technically navigating to the above URL does download the image, but if you want it saved on your computer, or in your Python workspace, you can once again use the `urllib2.urlopen()` function. The image data that is downloaded is just a string, and can be written out to file like text or anything else. It's a really big string, so dont try and print it. Instead, we should convert the string to a Python image with `Image()`, and use the IPython `display()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "web_response = urllib2.urlopen(data['data'][0]['attachments']['data'][0]['media']['image']['src'])\n",
    "image_data = web_response.read()\n",
    "image_object = Image.open(StringIO(image_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this will open the image in a window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_object.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And running this will place the display right here in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(image_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APIs usually handle many types of request\n",
    "So far we have set up to be able to gather a stream of pubic posts going back in time. As organizations (like Drexel) post public updates Facebook users will often comment, generating threaded discussion. Since these discussions are also public, we can access them. Let's look at one pose back so some comments will have had the chance to accumulate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "web_response = urllib.urlopen(data['data'][1]['attachments']['data'][0]['media']['image']['src'])\n",
    "image_data = web_response.read()\n",
    "image_object = Image.open(StringIO(image_data))\n",
    "print(data['data'][0]['message'])\n",
    "display(image_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey, this is about that neon sign museaum!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Facebook objects have unique identifiers\n",
    "To be able to request to comments associated to a post we will have to be able to provide the unique identifier for a post. Fortunately, this is provided!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['data'][1]['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating separate URL and request functions for comments\n",
    "Sadly, our first API-access function won't do for this type of request. Instead we will have to include a place for post IDs and specifically build a comments query. Note that there is also the 'filter' option for the comments, which ensures that all comments are returned in chronological order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPostComments(POST_ID, limit):\n",
    "    comments_url = createPostCommentsUrl(POST_ID, APP_ID, APP_SECRET, limit)\n",
    "    web_response = urllib2.urlopen(comments_url)\n",
    "    readable_page = web_response.read()\n",
    "    return json.loads(readable_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requesting comments\n",
    "Here, we will request the comments from the second to last post made by Drexel. Once again, there is paging information and the data. Once again, since we've requested multiple comments we have a list as a return object. Let's loop through the comments and print them out along with their `'created_time'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "comments_data = getPostComments(data['data'][1]['id'], 10)\n",
    "print(\"This post currently has \"+str(len(comments_data['data']))+\" comments. Here's what we got:\\n\")\n",
    "for comment in comments_data['data']:\n",
    "    print(comment[\"created_time\"])\n",
    "    print(comment[\"message\"])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The data emerge oldest to newest\n",
    "Note that all of these comments are from a few days ago and are getting newer. This is the reverse order of the posts feed, where we have to go back in time! Also, it appears people were more immediately interested with the move of firestone. My favorite comment is the 'glowing reviews' comment for the museaum..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter \n",
    "What if we want data from another source? Twitter has a similar API and actually makes much more of its data available than Facebook. However, it doesn't always have to be so difficult as constructing your very own API request URLs. In fact, python has several clients (modules) for downloading data from twitter that make the API access very easy! Here, we'll use `tweepy`. Since this is just a client, be aware that it may have limited functionality. So if you want to see everything that the API can do, check out the full documentation. However, like we were doing with the Facebook API, this may require building your own URLs.\n",
    "\n",
    "* https://dev.twitter.com/docs\n",
    "\n",
    "Just like with facebook, you'll have to get API access keys, which (from [stackoverflow](http://stackoverflow.com/questions/1808855/getting-new-twitter-api-consumer-and-secret-keys)) involves:\n",
    "\n",
    "1. Having a twitter account\n",
    "2. Go to https://apps.twitter.com and sign in.\n",
    "3. Create an app (fill out the form).\n",
    "4. Go To API keys section and click generate ACCESS TOKEN.\n",
    "\n",
    "Note that the resulting keys are refferred to as:\n",
    "\n",
    "* 'oauth_access_token' means **Access token**\n",
    "* 'oauth_access_token_secret' means **Access token secret**\n",
    "* 'consumer_key' means **API key**\n",
    "* 'consumer_secret' **means API secret**\n",
    "\n",
    "\n",
    "To get `tweepy`, just go to a command line and enter:\n",
    "\n",
    "```\n",
    "pip install tweepy\n",
    "```\n",
    "\n",
    "tweepy is pretty well documented, too:\n",
    "\n",
    "* http://docs.tweepy.org/en/v3.5.0/index.html\n",
    "* https://github.com/tweepy/tweepy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting started\n",
    "First things first, we will need to import the necessary modules and enter our access keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import json\n",
    "\n",
    "consumer_key=\"bGSPlAoZzbCFQfeQhxNmfj1cp\"\n",
    "consumer_secret=\"LR2DMvd3LffMIjYFPTQnlp036PgKlVEcn1rFqcWUWDEy2rFH2p\"\n",
    "access_token=\"227267417-D2IEgEgeUerDvbem0Of75nATQwbIiBXJDDJoVvVM\"\n",
    "access_token_secret=\"jxjiSsZfl2WJUgquSA7voZfEpoAJtbuP4vP28btCsYpbS\"\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The rest API\n",
    "The rest API allows you to access historical data (i.e., data that is 'resting') and to manage your account. This means looking up tweets by ID, and also follow/unfollow other accounts, etcetera. With tweepy, we first have to initialize a rest api instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rest = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading some old tweets\n",
    "To get some old tweets we will need a list of tweet IDs.\n",
    "\n",
    "Note: gathering the list of tweet IDs required going into the source html. After next week, we could write a web scraper to pull this out for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idlist = [\n",
    "    \"1121915133\", \n",
    "    \"64780730286358528\", \n",
    "    \"64877790624886784\", \n",
    "    \"20\", \n",
    "    \"467192528878329856\", \n",
    "    \"474971393852182528\",\n",
    "    \"475071400466972672\",\n",
    "    \"475121451511844864\",\n",
    "    \"440322224407314432\",\n",
    "    \"266031293945503744\",\n",
    "    \"3109544383\",\n",
    "    \"1895942068\",\n",
    "    \"839088619\",\n",
    "    \"8062317551\",\n",
    "    \"232348380431544320\",\n",
    "    \"286910551899127808\",\n",
    "    \"286948264236945408\",\n",
    "    \"27418932143\",\n",
    "    \"786571964\",\n",
    "    \"467896522714017792\",\n",
    "    \"290892494152028160\",\n",
    "    \"470571408896962560\"\n",
    "]\n",
    "data = {id_: \"\" for id_ in idlist}\n",
    "tweets = rest.statuses_lookup(id_=idlist, include_entities=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What does a tweet look like?\n",
    "The resulting status objects have a lot of extra structure to them, but a python dictionary of Twitter's raw format may be accessed through the `._json` value of the object. Let's look at the keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweets[0]._json.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important thing here is the `'text'`, but there's lots of other good stuff too. Let's look at all 19 of the tweets in order. Unfortunately, since the order is off, we will have to fix it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in tweets:\n",
    "    data[str(tweet._json['id'])] = tweet._json\n",
    "for ix, id_ in enumerate(idlist):\n",
    "    print(str(ix+1)+\": \"+data[id_]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting a user's timeline\n",
    "Now, we can also follow a specific user easily with tweepy. Let's get the last 10 tweets from Drexel (`drexeluniv`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeline = rest.user_timeline(screen_name = \"drexeluniv\", count = 10)\n",
    "for tweet in timeline:\n",
    "    print(tweet._json[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The streaming API\n",
    "So far we've only accessed the rest API for old tweets. Twitter is neat because it also makes its streaming API available to the public (at 1% bandwidth). Here's some mode advanced tweepy code that allows us to download `N` immediately recent tweets from the stream using keyword and geolocation filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StdOutListener(tweepy.streaming.StreamListener):\n",
    "    \"\"\" A listener handles tweets that are received from the stream.\n",
    "    This listener collects N tweets, storing them in memory, and then stops.\n",
    "    \"\"\"\n",
    "    def __init__(self, N):\n",
    "        super(StdOutListener,self).__init__(self)\n",
    "        self.data = []\n",
    "        self.N = N\n",
    "    def on_data(self, data):\n",
    "        self.data.append(json.loads(data))\n",
    "        if len(self.data) >= self.N:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNtweets(N, auth, track = [], locations = []):\n",
    "    listener = StdOutListener(N)\n",
    "    stream = tweepy.Stream(auth, listener)\n",
    "    if len(track) and len(locations):\n",
    "        stream.filter(track=track, locations = locations)\n",
    "    elif len(track):\n",
    "        stream.filter(track = track)\n",
    "    elif len(locations):\n",
    "        stream.filter(locations = locations)\n",
    "\n",
    "    return listener.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataScienceTweets = getNtweets(10, auth, track=['datascience'])\n",
    "for tweet in dataScienceTweets:\n",
    "    print(tweet['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geolocation data\n",
    "As mentioned above, we can also use the streaming API to filter data by location. Let's look at 10 recent tweets from Philadelphia! To do this, we will have to get a lat/lon bounding box for philadelphia. I got these number from\n",
    "\n",
    "* https://github.com/amyxzhang/boundingbox-cities/blob/master/boundbox.txt\n",
    "\n",
    "but as we will see below, we could gather this data from Google's API. Note: the lat/lon order for a location box is `[lon1,lat1,lon2,lat2]`. Note that because there are fewer tweets coming from such a small box, this will take a bit longer to run for 10 the tweets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox = [-75.280327, 39.864841, -74.941788, 40.154541]\n",
    "phillyTweets = getNtweets(10, auth, locations=bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phillyTweets[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in phillyTweets:\n",
    "    print(tweet['place']['full_name'])\n",
    "    print(tweet['text'])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google\n",
    "Google has API's for lot's of stuff. This includes all of the geographic features of maps, the linguistic features of translate, and even YouTube data, since Google bought them in 2006 for \\$1.65 billion. Here, we're just going to go forward and use a client that provides the geographic services. Like usual, you will have to have a Google account for this. The steps are then:\n",
    "\n",
    "1. Get a Google account.\n",
    "2. Get an API key: https://developers.google.com/places/web-service/get-api-key\n",
    "3. Go to the developer's console https://developers.google.com/console\n",
    "4. Enable the specific APIs of interest: https://support.google.com/cloud/answer/6158841?hl=en\n",
    "\n",
    "#### The python client\n",
    "Here, we're going to use a nice Python client for the maps services called googlemaps. We can install this easily from the command line with pip, once again:\n",
    "\n",
    "```\n",
    "pip install -U googlemaps\n",
    "```\n",
    "\n",
    "For more information, be sure to check out their project documentation:\n",
    "\n",
    "* https://github.com/googlemaps/google-maps-services-python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the client and set up your API instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import googlemaps\n",
    "from datetime import datetime\n",
    "\n",
    "GOOGLE_API_KEY = \"AIzaSyDFverCdXIh3_z7QdxMKIHhjfUxU1oavsc\"\n",
    "\n",
    "gmaps = googlemaps.Client(key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the geocoding for Rush and City halls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rushHall = gmaps.geocode('30 N. 33rd Street, Philadelphia, PA')\n",
    "print(rushHall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A bounding box for Rush hall!\n",
    "There's lots of information here about the building, but relating back to our Twitter API experiment, notice how we can actually get a bounding box for the building&mdash;this means we could download all of the tweets appearing from this building!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rushHall[0]['geometry']['viewport'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reverse lookup\n",
    "Note that we can also get the address of a location by lat/lon lookup! Let's see if we can pull the Rush hall address back out of the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look up an address with reverse geocoding\n",
    "lat = rushHall[0]['geometry']['location']['lat']\n",
    "lng = rushHall[0]['geometry']['location']['lng']\n",
    "reverseLookup = gmaps.reverse_geocode((lat, lng))\n",
    "for component in reverseLookup[0]['address_components']:\n",
    "    print(component['long_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Directions to city hall\n",
    "Google is great for driving directions and we can use the API for this, too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cityHall = gmaps.geocode('1401 John F Kennedy Blvd, Philadelphia, PA')\n",
    "\n",
    "# Request walking directions\n",
    "now = datetime.now()\n",
    "directions_result = gmaps.directions(\n",
    "    \"30 N. 33rd Street, Philadelphia, PA\",\n",
    "    \"Philadelphia City Hall\",\n",
    "    mode=\"driving\",\n",
    "    departure_time=now\n",
    ")\n",
    "print(directions_result[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What's the result?\n",
    "Once again, there's a lot of information here. Besides a list of lat/lon pairs for the directions (so you can make a map) there is also a text list of html directions in under the `'legs'` key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"It's a \"+directions_result[0]['legs'][0]['distance']['text']+\" walk, total:\\n\")\n",
    "stepnum = 1\n",
    "for step in directions_result[0]['legs'][0]['steps']:\n",
    "    print(str(stepnum)+\") \"+re.sub(\"<\\/?b>\", \"\", step['html_instructions']))\n",
    "    stepnum += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## A more local example of an API\n",
    "The Southeastern Pennsylvania Transportation Authority (SEPTA) [makes a few APIs available](http://www3.septa.org/hackathon/). Some of these APIs can be used to access realtime data about SEPTA transit (trains, buses, trolleys). For example, we can request data about the next trains to arrive at a given station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'30th Street Station Departures: April 13, 2022, 5:29 pm': [{'Northbound': [{'direction': 'N',\n",
       "     'path': 'R0N',\n",
       "     'train_id': '1084',\n",
       "     'origin': 'Cynwyd',\n",
       "     'destination': 'Suburban Sta',\n",
       "     'line': 'Manayunk/Norristown',\n",
       "     'status': 'On Time',\n",
       "     'service_type': 'LOCAL',\n",
       "     'next_station': '30th St',\n",
       "     'sched_time': '2022-04-13 17:31:01.000',\n",
       "     'depart_time': '2022-04-13 17:32:00.000',\n",
       "     'track': '1',\n",
       "     'track_change': None,\n",
       "     'platform': '',\n",
       "     'platform_change': None},\n",
       "    {'direction': 'N',\n",
       "     'path': 'R3N',\n",
       "     'train_id': '6336',\n",
       "     'origin': '30th Street Station',\n",
       "     'destination': 'West Trenton',\n",
       "     'line': 'West Trenton',\n",
       "     'status': 'On Time',\n",
       "     'service_type': 'EXP TO JENKINTOWN',\n",
       "     'next_station': None,\n",
       "     'sched_time': '2022-04-13 17:36:01.000',\n",
       "     'depart_time': '2022-04-13 17:37:00.000',\n",
       "     'track': '2',\n",
       "     'track_change': None,\n",
       "     'platform': '',\n",
       "     'platform_change': None},\n",
       "    {'direction': 'N',\n",
       "     'path': 'R5/4N',\n",
       "     'train_id': '5442',\n",
       "     'origin': 'Malvern',\n",
       "     'destination': 'Glenside',\n",
       "     'line': 'Paoli/Thorndale',\n",
       "     'status': 'On Time',\n",
       "     'service_type': 'LOCAL',\n",
       "     'next_station': 'Overbrook',\n",
       "     'sched_time': '2022-04-13 17:39:01.000',\n",
       "     'depart_time': '2022-04-13 17:40:00.000',\n",
       "     'track': '1',\n",
       "     'track_change': None,\n",
       "     'platform': '',\n",
       "     'platform_change': None},\n",
       "    {'direction': 'N',\n",
       "     'path': 'R7N',\n",
       "     'train_id': '722',\n",
       "     'origin': 'Trenton',\n",
       "     'destination': 'Chestnut H East',\n",
       "     'line': 'Trenton',\n",
       "     'status': '2 min',\n",
       "     'service_type': 'LOCAL',\n",
       "     'next_station': 'Tacony',\n",
       "     'sched_time': '2022-04-13 17:46:01.000',\n",
       "     'depart_time': '2022-04-13 17:47:00.000',\n",
       "     'track': '2',\n",
       "     'track_change': None,\n",
       "     'platform': '',\n",
       "     'platform_change': None},\n",
       "    {'direction': 'N',\n",
       "     'path': 'R5N',\n",
       "     'train_id': '6598',\n",
       "     'origin': '30th Street Station',\n",
       "     'destination': 'Doylestown',\n",
       "     'line': 'Lansdale/Doylestown',\n",
       "     'status': 'On Time',\n",
       "     'service_type': 'LOCAL',\n",
       "     'next_station': None,\n",
       "     'sched_time': '2022-04-13 17:47:00.000',\n",
       "     'depart_time': '2022-04-13 17:48:00.000',\n",
       "     'track': '1',\n",
       "     'track_change': None,\n",
       "     'platform': '',\n",
       "     'platform_change': None}]},\n",
       "  {'Southbound': [{'direction': 'S',\n",
       "     'path': 'R4/2S',\n",
       "     'train_id': '9225',\n",
       "     'origin': '30th Street Station',\n",
       "     'destination': 'Wilmington',\n",
       "     'line': 'Wilmington/Newark',\n",
       "     'status': 'On Time',\n",
       "     'service_type': 'LOCAL',\n",
       "     'next_station': '30th St',\n",
       "     'sched_time': '2022-04-13 17:31:01.000',\n",
       "     'depart_time': '2022-04-13 17:32:00.000',\n",
       "     'track': '6',\n",
       "     'track_change': None,\n",
       "     'platform': '',\n",
       "     'platform_change': None},\n",
       "    {'direction': 'S',\n",
       "     'path': 'R5S',\n",
       "     'train_id': '6525',\n",
       "     'origin': 'Jefferson Station',\n",
       "     'destination': '30th St',\n",
       "     'line': 'Paoli/Thorndale',\n",
       "     'status': '2 min',\n",
       "     'service_type': 'LOCAL',\n",
       "     'next_station': 'Jefferson',\n",
       "     'sched_time': '2022-04-13 17:38:01.000',\n",
       "     'depart_time': '2022-04-13 17:39:00.000',\n",
       "     'track': '',\n",
       "     'track_change': None,\n",
       "     'platform': '',\n",
       "     'platform_change': None},\n",
       "    {'direction': 'S',\n",
       "     'path': 'R5S',\n",
       "     'train_id': '9553',\n",
       "     'origin': 'Jefferson Station',\n",
       "     'destination': 'Malvern',\n",
       "     'line': 'Paoli/Thorndale',\n",
       "     'status': 'On Time',\n",
       "     'service_type': 'LOCAL',\n",
       "     'next_station': 'Jefferson',\n",
       "     'sched_time': '2022-04-13 17:39:01.000',\n",
       "     'depart_time': '2022-04-13 17:40:00.000',\n",
       "     'track': '4',\n",
       "     'track_change': None,\n",
       "     'platform': '',\n",
       "     'platform_change': None},\n",
       "    {'direction': 'S',\n",
       "     'path': 'R3S',\n",
       "     'train_id': '9349',\n",
       "     'origin': 'Temple U',\n",
       "     'destination': 'Elwyn',\n",
       "     'line': 'Media/Elwyn',\n",
       "     'status': '4 min',\n",
       "     'service_type': 'LOCAL',\n",
       "     'next_station': 'Temple U',\n",
       "     'sched_time': '2022-04-13 17:41:01.000',\n",
       "     'depart_time': '2022-04-13 17:42:00.000',\n",
       "     'track': '6',\n",
       "     'track_change': None,\n",
       "     'platform': '',\n",
       "     'platform_change': None},\n",
       "    {'direction': 'S',\n",
       "     'path': 'R7S',\n",
       "     'train_id': '725',\n",
       "     'origin': 'Temple U',\n",
       "     'destination': 'Trenton',\n",
       "     'line': 'Trenton',\n",
       "     'status': '1 min',\n",
       "     'service_type': 'LOCAL',\n",
       "     'next_station': 'Temple U',\n",
       "     'sched_time': '2022-04-13 17:43:01.000',\n",
       "     'depart_time': '2022-04-13 17:44:00.000',\n",
       "     'track': '3',\n",
       "     'track_change': None,\n",
       "     'platform': '',\n",
       "     'platform_change': None}]}]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# format: \"http://www3.septa.org/hackathon/Arrivals/*STATION_NAME*/*NUMBER_OF_TRAINS*\"\n",
    "arrivals_response = requests.get(\"http://www3.septa.org/hackathon/Arrivals/30th Street Station/5\")\n",
    "\n",
    "arrivals_dict = arrivals_response.json()\n",
    "arrivals_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a request to the SEPTA Arrivals API to get data on the next 10 trains to arrive at Suburban Station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'direction': 'N',\n",
      "  'line': 'Warminster',\n",
      "  'sched_time': '2022-04-13 17:30:00.000',\n",
      "  'status': '2 min',\n",
      "  'track': '2'},\n",
      " {'direction': 'N',\n",
      "  'line': 'Fox Chase',\n",
      "  'sched_time': '2022-04-13 17:34:00.000',\n",
      "  'status': '1 min',\n",
      "  'track': '1'},\n",
      " {'direction': 'N',\n",
      "  'line': 'Manayunk/Norristown',\n",
      "  'sched_time': '2022-04-13 17:36:00.000',\n",
      "  'status': 'On Time',\n",
      "  'track': '6'},\n",
      " {'direction': 'N',\n",
      "  'line': 'West Trenton',\n",
      "  'sched_time': '2022-04-13 17:41:00.000',\n",
      "  'status': 'On Time',\n",
      "  'track': '2'},\n",
      " {'direction': 'N',\n",
      "  'line': 'Paoli/Thorndale',\n",
      "  'sched_time': '2022-04-13 17:44:00.000',\n",
      "  'status': 'On Time',\n",
      "  'track': '1'},\n",
      " {'direction': 'N',\n",
      "  'line': 'Trenton',\n",
      "  'sched_time': '2022-04-13 17:51:00.000',\n",
      "  'status': '2 min',\n",
      "  'track': '2'},\n",
      " {'direction': 'N',\n",
      "  'line': 'Lansdale/Doylestown',\n",
      "  'sched_time': '2022-04-13 17:52:00.000',\n",
      "  'status': 'On Time',\n",
      "  'track': '1'},\n",
      " {'direction': 'N',\n",
      "  'line': 'Media/Elwyn',\n",
      "  'sched_time': '2022-04-13 17:56:00.000',\n",
      "  'status': 'On Time',\n",
      "  'track': '1'},\n",
      " {'direction': 'N',\n",
      "  'line': 'Airport',\n",
      "  'sched_time': '2022-04-13 18:04:00.000',\n",
      "  'status': 'On Time',\n",
      "  'track': '2'},\n",
      " {'direction': 'N',\n",
      "  'line': 'Wilmington/Newark',\n",
      "  'sched_time': '2022-04-13 18:15:00.000',\n",
      "  'status': '1 min',\n",
      "  'track': '1'},\n",
      " {'direction': 'S',\n",
      "  'line': 'Paoli/Thorndale',\n",
      "  'sched_time': '2022-04-13 17:34:00.000',\n",
      "  'status': '2 min',\n",
      "  'track': '4'},\n",
      " {'direction': 'S',\n",
      "  'line': 'Paoli/Thorndale',\n",
      "  'sched_time': '2022-04-13 17:35:00.000',\n",
      "  'status': 'On Time',\n",
      "  'track': '4'},\n",
      " {'direction': 'S',\n",
      "  'line': 'Media/Elwyn',\n",
      "  'sched_time': '2022-04-13 17:37:00.000',\n",
      "  'status': '2 min',\n",
      "  'track': '4'},\n",
      " {'direction': 'S',\n",
      "  'line': 'Trenton',\n",
      "  'sched_time': '2022-04-13 17:39:00.000',\n",
      "  'status': 'On Time',\n",
      "  'track': '3'},\n",
      " {'direction': 'S',\n",
      "  'line': 'Chestnut Hill West',\n",
      "  'sched_time': '2022-04-13 17:47:00.000',\n",
      "  'status': 'On Time',\n",
      "  'track': '3'},\n",
      " {'direction': 'S',\n",
      "  'line': 'West Trenton',\n",
      "  'sched_time': '2022-04-13 17:50:00.000',\n",
      "  'status': '1 min',\n",
      "  'track': '3'},\n",
      " {'direction': 'S',\n",
      "  'line': 'Cynwyd',\n",
      "  'sched_time': '2022-04-13 17:53:30.000',\n",
      "  'status': 'On Time',\n",
      "  'track': '6'},\n",
      " {'direction': 'S',\n",
      "  'line': 'Fox Chase',\n",
      "  'sched_time': '2022-04-13 17:54:00.000',\n",
      "  'status': '23 min',\n",
      "  'track': '3'},\n",
      " {'direction': 'S',\n",
      "  'line': 'Manayunk/Norristown',\n",
      "  'sched_time': '2022-04-13 17:58:00.000',\n",
      "  'status': '2 min',\n",
      "  'track': '4'},\n",
      " {'direction': 'S',\n",
      "  'line': 'Lansdale/Doylestown',\n",
      "  'sched_time': '2022-04-13 18:00:00.000',\n",
      "  'status': '1 min',\n",
      "  'track': '4'}]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pprint import pprint\n",
    "\n",
    "response = requests.get(\"http://www3.septa.org/hackathon/Arrivals/Suburban Station/10\")\n",
    "\n",
    "data = response.json()\n",
    "top_keys = list(data.keys())\n",
    "# pprint(data[top_keys[0]][0][\"Northbound\"])\n",
    "\n",
    "trains = []\n",
    "for timestamp in data: ## timestamp is the sole key at the top level of response\n",
    "    for outbound_direction in data[timestamp]: ## each track direction gets its own dictionary\n",
    "        for direction in outbound_direction:\n",
    "            for train in outbound_direction[direction]:\n",
    "                trains.append({\n",
    "                    'direction': train['direction'],\n",
    "                    'line': train['line'],\n",
    "                    'sched_time': train['sched_time'],\n",
    "                    'status': train['status'],\n",
    "                    'track': train['track']\n",
    "                })\n",
    "\n",
    "pprint(trains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
